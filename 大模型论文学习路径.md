

# **大型语言模型（LLM）关键论文与技术知识库**

本报告旨在系统性地梳理和总结大型语言模型（LLM）领域的关键论文与核心技术。报告将按照技术演进脉络，分为奠基性架构、缩放法则、微调与对齐、推理与智能体、领域应用及底层基础设施六大类别，并对每一类别的核心论文进行逐一的翻译、汇总与深度解析。

## **一、 奠基性架构与范式**

本章追溯大型模型的架构起源，从 Transformer 的提出，到预训练范式的两大分支（BERT的编码器与GPT的解码器），再到现代开源基石（Llama 3）的出现。

### **1.1 Transformer 架构： "Attention Is All You Need" (2017)**

该论文 1 标志着现代深度学习序列处理的转折点。

* **核心论点：** 首次提出完全摒弃传统序列处理中根深蒂固的循环（Recurrence，如 RNN/LSTM）和卷积（Convolution）结构，论证了仅依赖“注意力机制”（Attention）的 Transformer 架构，在机器翻译任务上更具并行化能力，并且能更有效地捕捉长距离依赖 3。  
* **实现细节：**  
  * **架构：** 采用编码器-解码器（Encoder-Decoder）堆叠结构。编码器和解码器均由 $N$ 层相同的层堆叠而成 4。  
  * **核心机制（Scaled Dot-Product Attention）：** 这是注意力的核心。它通过三个可学习的权重矩阵，将输入（词嵌入）投影为三个向量：查询（Query, $Q$）、键（Key, $K$）和值（Value, $V$） 4。注意力函数的输出是 $V$ 的加权和，权重由 $Q$ 和 $K$ 的点积相似度（经 $d\_k$ 缩放）通过 Softmax 计算得出。  
  * **多头注意力（Multi-Head Attention）：** 论文并未止步于单一的注意力。它将 $Q$, $K$, $V$ 线性投影 $h$ 次（$h$ 为“头”数），每个头并行执行一次注意力计算。随后，将 $h$ 个头的输出拼接（Concat）并再次进行线性投影 1。其动机是，单一注意力头会抑制模型的能力，而多头允许模型在不同位置共同关注来自不同“表示子空间”（representation subspaces）的信息 1。  
  * **关键组件：** 在每个子层（即多头注意力和前馈网络FFN）周围使用了残差连接（Residual connection），随后进行层归一化（Layer Normalization） 4。  
  * **正则化：** 训练期间，在每个子层的输出（添加到残差输入和归一化之前）以及嵌入和位置编码的总和上，都应用了 $P\_{drop} \= 0.1$ 的 Dropout 4。  
* **实验与论证：**  
  * **任务：** WMT 2014 英德和英法机器翻译任务 1。  
  * **结果：** "Transformer (big)" 模型在英德翻译上达到了 28.4 的 BLEU 分数，在英法翻译上达到了 41.0 的 BLEU 分数，显著超越了当时所有的SOTA模型 1。  
  * **训练细节：** 论文的评审者在当时的评审意见中指出，该模型的超参数（特别是学习率调度机制）对其成功至关重要，而传统的随机梯度下降（SGD）方法在此模型上似乎会失效 5。

### **1.2 双向预训练： "BERT: Pre-training of Deep Bidirectional Transformers" (2018)**

BERT 6 的出现，正式确立了“预训练-微调”（Pre-training, Fine-tuning）的范式，并展示了深度双向模型在自然语言理解（NLU）上的威力。

* **核心论点：** 论证了先前模型（如 GPT-1）的局限性在于其“单向性”（unidirectionality），这限制了模型在预训练期间的表征能力。BERT通过使用“掩码语言模型”（MLM）任务，实现了真正的双向上下文表征 6。  
* **实现细节：**  
  * **架构：** BERT 仅使用了 Transformer 的编码器（Encoder）部分 8。  
  * **预训练任务 1 (MLM)：** 掩码语言模型（Masked Language Model）。在预训练期间，随机遮蔽（Mask）输入序列中 15% 的 Token，模型的目标是仅基于其上下文（即左右两侧的 Token）预测被遮蔽 Token 的原始词汇 ID 6。  
  * **预训练任务 2 (NSP)：** 下一句预测（Next Sentence Prediction）。训练模型判断两个句子（A和B）是否是连续的，以此来理解句子间的关系 6。  
  * **模型规模：** 实现了两种尺寸：$BERT\_{BASE}$（1.1亿参数）和 $BERT\_{LARGE}$（3.4亿参数），两者均在 Toronto BookCorpus 和英文维基百科上进行训练 10。  
* **实验与论证：**  
  * **任务：** BERT 在 11 项 NLP 任务上进行了评估，包括 GLUE（9个任务）、SQuAD v1.1 和 v2.0、SWAG 6。  
  * **结果：** 该模型在所有 11 项任务上均取得了当时的 SOTA（State-of-the-Art）结果 7。例如，GLUE 得分达到 80.5%，SQuAD v1.1 F1 达到 93.2 7。  
  * **微调实现：** 对于分类任务，BERT 的微调实现非常简洁：将特殊的 Token 对应的最终隐藏状态输入到一个简单的线性-Softmax 层中进行预测 10。

### **1.3 自回归与上下文学习： "GPT-3: Language Models are Few-Shot Learners" (2020)**

GPT-3 11 的发布，其核心贡献不在于架构创新，而在于揭示了“规模”（Scale）本身可以解锁全新的模型能力——“上下文学习”（In-Context Learning）。

* **核心论点：** 提出了“上下文学习”（In-Context Learning, ICL）范式 12。论文论证了，通过极大地扩大模型规模，模型无需任何梯度更新或微调，仅通过在推理时提供少量（Few-shot）任务示例作为提示（Prompt），就能学会解决新任务 12。  
* **实现细节：**  
  * **架构：** （由 13 隐含）基于 Transformer 解码器（Decoder-only）架构，采用自回归（Autoregressive）的预训练目标（即预测下一个词）。  
  * **ICL 范式：**  
    * **Zero-shot：** 仅提供任务描述，不提供示例。  
    * **One-shot：** 提供一个任务示例。  
    * **Few-shot：** 提供多个（如10-100个）任务示例 11。  
* **实验与论证：**  
  * **语言建模：** 在 Penn Tree Bank (PTB) 数据集上，GPT-3 的最大模型实现了新的 SOTA，困惑度（Perplexity）相比先前 SOTA 降低了 15 个点 14。  
  * **下游任务：** 在 LAMBADA 数据集（需要模型预测段落的最后一个词）上表现出色 14。  
  * **局限性分析（关键）：** 论文坦诚地指出，尽管 ICL 表现出色，但在某些特定任务上（特别是涉及比较的任务，如 WIC 和 ANLI），即使是 Few-shot 设置，其性能也仅略优于随机猜测 11。

架构演进分析：  
Transformer 1 的提出（2017年）提供了一个完整的编码器-解码器架构，专为机器翻译等序列到序列任务设计。然而，随后的研究立即将其拆分：BERT (2018) 6 仅使用了编码器部分，并通过 MLM 预训练，专注于自然语言理解（NLU）；而 GPT 系列 (2018-2020) 12 则仅使用了解码器部分，通过自回归目标专注于自然语言生成（NLG）。  
这种架构的分裂导致了 NLU 和 NLG 两个独立的研究分支。然而，GPT-3 12 的 ICL 实验（如 13 所示）证明，NLG 模型（解码器）可以通过 ICL 范式*解决* NLU 任务（如问答、分类）。这最终导致了*解码器-Only* 架构在功能上统一了所有 NLP 任务，使其成为后续（如 Llama 3）的主流选择。

### **1.4 开源基石： "Llama 3: The Llama 3 Herd of Models" (2024)**

Llama 3 15 代表了当前开源大型模型的 SOTA 水平，其成功不仅在于模型本身，更在于其推动的开源生态。

* **核心发布：** Meta 公开发布了 Llama 3 系列模型，包括 8B 和 70B 版本（以及论文中提到的 405B 参数的预训练和后训练版本）15。  
* **实现细节：**  
  * **安全：** 发布了 Llama Guard 3 模型，用于对模型的输入和输出进行安全过滤 15。  
  * **多模态：** 论文中展示了通过“组合式方法”（compositional approach）将图像、视频和语音能力集成到 Llama 3 中的实验 15。  
* 实验与论证（生态与微调）：  
  GPT-3 的巨大规模（175B）使得全参数微调变得极其昂贵 18，这直接催生了对参数高效微调（PEFT）技术的需求 19。Llama 3 的发布恰好是这场 PEFT 革命的受益者和推动者。  
  * 一个关键的实现案例 17 展示了如何利用 PEFT 中的 LoRA 技术（详见 3.1.2 节）对 Llama 3 8B 模型进行微调（任务为虚假新闻事实核查）。  
  * **实验环境：** 该实验在 Hugging Face 提供的 NVIDIA A10G 计算环境（消费级/企业级GPU）上进行 17。  
  * **训练细节：** 仅训练 4 个 epoch，使用梯度累积步数为 4，最大梯度范数（max gradient norm）设为 1.0 以防止不稳定 17。  
  * **评估：** 在基准测试中，Llama 3 在人工评估的熟练度考试基准（Proficiency Exam Evaluations）上表现优异 21。

## **二、 模型的缩放与效率**

本章探讨 LLM 发展的两个核心“张力”：如何通过扩大规模（Scaling）提升能力，以及如何通过架构创新（如 MoE）在不增加计算（Compute）的前提下扩大规模。

### **2.1 缩放法则 (Scaling Laws)**

#### **2.1.1 OpenAI (Kaplan) 的缩放法则 (2020)**

"Scaling Laws for Neural Language Models" 22 是该领域的奠基之作。

* **核心论点：** 模型性能（以 Test Loss $L$ 衡量）可预测地依赖于三个因素：模型大小（$N$，非嵌入参数数量）、数据集大小（$D$）和计算预算（$C$，FLOPs）。这种关系遵循平滑的幂律（Power-law）22。  
* **关键发现：**  
  1. **规模 \> 形状：** 性能与模型规模（$N$）强相关，而与网络宽度或深度等“形状”因素在很大范围内关系不大 22。  
  2. **样本效率：** 更大的模型更具“样本效率”（Sample-Efficient）22。  
* **核心推论：** 在计算预算有限的情况下，最优策略是训练一个**非常大**的模型，并使用**相对适中**的数据量，且在模型**远未收敛时**就停止训练 22。

#### **2.1.2 DeepMind (Chinchilla) 的修正 (2022)**

"Training Compute-Optimal Large Language Models" 26 对 OpenAI 的结论提出了根本性的修正。

* **核心论点：** OpenAI (2020) 的结论是错误的。当前的大型模型（如 Gopher, GPT-3）均处于\*\*“严重训练不足”（Significantly Undertrained）\*\*的状态 26。  
* **实验设计：**  
  * 为了找到固定计算预算（FLOPs）下的最优 $N$ 和 $D$，研究人员训练了**超过 400 个**语言模型 26。  
  * 参数范围：70M 到 16B 26。  
  * 数据范围：5B 到 500B Tokens 26。  
* **新缩放法则：**  
  * 为了实现计算最优（Compute-optimal），**模型大小（$N$）和训练数据量（$D$）应该等比例缩放** 26。  
  * 具体而言：模型大小每翻一倍，训练 Token 数量也应翻一倍 26。  
* **验证实验：**  
  * **基线 (Gopher)：** DeepMind 之前的模型 Gopher (280B 参数) 使用了约 300B Tokens。  
  * **Chinchilla (70B)：** 使用与 Gopher *相同*的计算预算，但按照新法则调整：参数量**减少 4 倍** (70B)，训练数据量**增加 4 倍** (1.4T Tokens) 26。  
  * **结果：** 70B 的 Chinchilla 在所有下游基准上（如 MMLU 提升 7%）**全面且显著地优于** 280B 的 Gopher、175B 的 GPT-3 和 530B 的 MT-NLG 26。

缩放法则的影响分析：  
Chinchilla 26 的发现是对行业的一次重大修正。OpenAI (2020) 22 的“模型优先”观点导致了对模型（$N$）的过度投资，而 Gopher 和 GPT-3 等模型实际上浪费了大量计算资源在“训练不足”的模型上。Chinchilla 26 证明了数据（$D$）在计算最优性上与模型（$N$）同等重要。这一发现将行业的瓶颈从“如何构建更大的模型”转向了“如何获取更多的高质量训练数据”。

### **2.2 稀疏化架构：混合专家 (MoE)**

缩放法则表明 $N$（参数量）越大越好，但这带来了高昂的推理成本。混合专家（MoE）架构旨在解决这一矛盾。

#### **2.2.1 MoE 核心概念**

* **定义：** MoE 是一种机器学习技术。它不使用单一（稠密）模型处理所有输入，而是使用一个“门控网络”（Gating Model）或“路由器”（Router）将输入动态地路由到一个或多个“专家网络”（Expert Networks，即子模型）29。  
* **工作原理：** MoE 将 Transformer 层中的 FFN（前馈网络）层替换为 MoE 层。MoE 层包含 $N$ 个专家（例如 8 个），每个专家本身就是一个 FFN 30。门控网络决定将每个 Token 发送给哪些专家 30。  
* **优势（稀疏激活）：** 模型的**总参数量**（$N\_{total}$）可以非常大（例如 $N \\times$ FFN 参数），但每个 Token 的**激活参数量**（$N\_{active}$）或计算成本（FLOPs）保持不变（仅等于其被路由到的专家，例如1个或2个 FFN）31。

#### **2.2.2 Switch Transformers (Google, 2021\)**

"Switch Transformers: Scaling to Trillion Parameter Models" 31 解决了 MoE 训练不稳定的问题。

* **核心论点：** MoE 的广泛采用受到复杂性、通信成本和训练不稳定的阻碍 31。  
* **实现细节（简化 MoE）：**  
  * **Top-k 路由：** 传统 MoE（如 GShard）将 Token 路由到 Top-k（$k \\ge 2$）个专家 32。  
  * **Switch 路由 (Top-1)：** Switch Transformer 提出了一种简化的\*\*“单专家策略”（$k=1$）\*\*。Token 仅被路由到*一个*专家 30。  
  * 优势 32： (1) 路由计算减少；(2) 每个专家的批量大小（Batch size）减半；(3) 通信成本降低；(4) 模型质量得以保留。  
* **实验与论证：**  
  * **低精度训练：** 首次展示了大型稀疏模型（MoE）可以使用 **bfloat16** 低精度格式进行稳定训练 31。  
  * **性能：** 基于 T5-Base 和 T5-Large 设计的模型，在相同计算资源下，预训练速度提升高达 **7 倍** 31。  
  * **规模：** 成功预训练了高达\*\*万亿（Trillion）\*\*参数的模型 31。

#### **2.2.3 Mixtral (Mistral AI, 2024\)**

"Mixtral of Experts" 34 是 MoE 架构在开源社区的 SOTA 实现。

* **核心论点：** 推出 Mixtral 8x7B，一个高性能的稀疏混合专家模型 (SMoE)，性能优于 Llama 2 70B 34。  
* **实现细节：**  
  * **架构：** Mixtral 8x7B 是一个 Decoder-only 模型，每层有 8 个 FFN 专家（总参数 46.7B）35。  
  * **路由策略：** Mixtral 采用了\*\*“Top-2 路由”\*\* 35。门控网络（一个线性层+Softmax）为每个 Token 选择两个最合适的专家。Token 被发送到这两个专家，它们的输出通过门控网络的权重进行加权求和 35。  
  * **效率：** 尽管总参数量巨大，但每个 Token 在推理时仅使用约 12.9B 的激活参数（$N\_{active}$）35，使其推理速度与 13B 稠密模型相当。  
* **实验与论证：**  
  * **性能：** 在数学（Math）和代码（Code）基准上显著优于 Llama 2 70B 35。  
  * **上下文处理：** 实验声称，Mixtral 能在 32k 的上下文窗口中成功检索信息，**无论序列长度和信息在序列中的位置如何** 34。

效率架构分析：  
MoE 31 的出现，为缩放法则 26 带来的挑战提供了一个巧妙的解决方案。它允许模型在“参数-性能”曲线上取得高位（由 $N\_{total}$ 决定），同时在“计算-成本”曲线上保持低位（由 $N\_{active}$ 决定）。然而，Mixtral 34 关于长上下文（32k）鲁棒性的声明，与 "Lost in the Middle" 37（详见 4.1.3 节）的发现（即模型在上下文中间位置性能会显著下降）形成了鲜明对比。这揭示了一个潜在的矛盾：是 Mixtral 的 MoE 架构（Top-2 路由）天然缓解了“注意力丢失”问题，还是其内部评估不够严格？

### **2.3 涌现能力的探讨**

* **核心论点：** “涌现能力”（Emergent Abilities）是指模型在规模较小时表现不佳（如随机猜测），但当模型规模达到某个阈值时，性能会突然“跃升”的现象 38。  
* **论证与反思：** GPT-2 和 GPT-3 的实验 38 显示，随着规模扩大，模型产生了质的新能力（如生成连贯长文本）。然而，"Are Emergent Abilities a Mirage?" 39 提出，这些“涌现”可能只是一种\*\*“幻觉”（Mirage）**，它是由研究者选择的**特定度量指标（Metric）\*\*（如准确率）人为造成的。如果选择更平滑（smooth）的度量指标（如对数似然），性能几乎总是随规模呈一致、可预测的趋势 39。

---

**表 1：缩放法则与模型效率对比**

| 论文 / 模型 | 核心论点 | N (参数) vs D (数据) 关系 | 关键实验/实现 | 结论/影响 |
| :---- | :---- | :---- | :---- | :---- |
| **OpenAI Law (2020)** 22 | 性能是 $N, D, C$ 的幂律。 | $N$ 的影响大于 $D$。应训练大模型、适中数据、提前停止。 | 拟合 $L(N, D, C)$ 曲线。 | 导致“模型优先”的军备竞赛（如 GPT-3）。 |
| **Chinchilla (2022)** 26 | 当前模型“训练不足”。 | $N$ 和 $D$ 应**等比例**缩放。 | 训练 400+ 模型。70B (1.4T) 模型击败 280B (300B) 模型。 | 纠正了缩放法则，转向“数据优先”。 |
| **Switch-T (2021)** 31 | 简化 MoE 以实现稳定和速度。 | 解耦 $N\_{total}$ 和 $N\_{active}$。 | **Top-1 路由**；bfloat16 训练；万亿参数模型。 | 证明了稀疏万亿参数模型的可行性；速度提升 7x。 |
| **Mixtral (2024)** 34 | SOTA 性能的开源 SMoE。 | 解耦 $N\_{total}$ 和 $N\_{active}$。 | **Top-2 路由**；8x7B (激活 $\\approx$ 13B)；32k 上下文。 | 性能超越 Llama 2 70B (稠密)。 |

## **三、 参数高效微调与指令对齐**

本章探讨 LLM 应用的两个关键阶段：如何*高效地*使其适应特定任务（PEFT），以及如何*安全地*使其行为符合人类意图（Alignment）。

### **3.1 参数高效微调 (PEFT)**

#### **3.1.1 PEFT 核心概念**

* **动机：** 全参数微调（Full fine-tuning）大型模型（如 GPT-3 175B）成本高昂（需要数百 GB 显存）、存储开销大（每个任务都需保存一个 175B 模型的副本）18，且易导致“灾难性遗忘”（Catastrophic Forgetting）。  
* **目标：** 冻结绝大多数（如 99%）的预训练参数，仅调整一小部分“适配器”（Adapter）参数 19。  
* **优势：** 显著降低计算和内存需求 19；降低过拟合风险 20；易于切换任务（只需替换小型适配器层）19。

#### **3.1.2 LoRA: Low-Rank Adaptation (2021)**

"LoRA: Low-Rank Adaptation Of Large Language Models" 18 是目前最主流的 PEFT 技术。

* **核心论点：** 研究发现，预训练模型权重的*变化*（$\\Delta W$）在微调过程中具有很低的“内在秩”（intrinsic rank）18。  
* **实现细节：**  
  * **架构：** LoRA 不直接修改原始权重 $W$（$W$ 被冻结）。它在目标层（如 Transformer 的注意力层 41）旁边，注入两个可训练的“秩分解矩阵”（Rank Decomposition Matrices）$A$ 和 $B$。  
  * **计算：** 模型的前向传播变为 $h \= Wx \+ \\Delta Wx \= Wx \+ (BA)x$ 42。  
  * **维度：** 假设 $W \\in \\mathbb{R}^{d \\times k}$，LoRA 使用 $A \\in \\mathbb{R}^{d \\times r}$ 和 $B \\in \\mathbb{R}^{r \\times k}$（注：42 中 $A, B$ 顺序相反，但原理一致），其中秩 $r \\ll \\min(d, k)$。可训练参数从 $d \\times k$ 降为 $(d+k) \\times r$。  
  * 超参数 41： r（秩）和 lora\_alpha（LoRA 缩放因子）。  
* **实验与论证：**  
  * **效率：** 与 Adam 全微调 GPT-3 175B 相比，LoRA 可将可训练参数数量减少 **10,000 倍**，GPU 内存需求减少 **3 倍** 18。  
  * **性能：** LoRA 在 RoBERTa, DeBERTa, GPT-2, GPT-3 上的模型质量与全微调相当或更好 18。  
  * **推理延迟：** LoRA 在部署时，可将 $BA$ 矩阵与 $W$ 矩阵相加（$W' \= W \+ BA$），不引入**任何**额外的推理延迟 18。

#### **3.1.3 QLoRA: Efficient Finetuning of Quantized LLMs (2023)**

"QLoRA: Efficient Finetuning of Quantized LLMs" 43 进一步降低了微调的硬件门槛。

* **核心论点：** LoRA 18 减少了可训练参数，但仍需在内存中加载完整的 16-bit 模型权重，内存开销（VRAM）依然巨大。QLoRA 旨在通过量化基础模型，进一步降低内存需求 44。  
* 实现细节（三大创新）44：  
  1. **4-bit NormalFloat (NF4)：** 一种新的 4-bit 数据类型，论文论证其对于正态分布的权重（Weights）是信息论上最优的。  
  2. **双重量化 (Double Quantization)：** 对量化常数（Quantization Constants）本身再次进行量化，以进一步减少内存占用（平均每个参数节省 0.38 bits）。  
  3. **分页优化器 (Paged Optimizers)：** 解决内存峰值（Spikes）问题。当 GPU 显存不足（OOM）时，自动将优化器状态“分页”（Evict）到 CPU 内存，并在需要时取回 45。  
* **实验与论证：**  
  * **核心结果：** QLoRA 使得在**单个 48GB GPU** 上微调一个 **65B** 参数模型成为可能 44。  
  * **模型（Guanaco）：** QLoRA 微调的 Guanaco 模型家族，在 Vicuna 基准测试中达到了 ChatGPT 99.3% 的性能水平，优于所有其他开源模型 44。  
  * **性能：** QLoRA 保持了 16-bit LoRA 微调的完整性能 44。

PEFT 演进分析：  
从 LoRA 18 到 QLoRA 44 的演进，体现了“效率”到“可及性”（Accessibility）的转变。LoRA 解决了实验室的效率问题（在同等资源下运行更多实验）；而 QLoRA 解决了可及性问题 46，它将微调 65B 模型的硬件门槛从“DGX A100 服务器”降低到“单张 48GB GPU”（如 A6000 或消费级 RTX 4090）44。这种“民主化”是 2023 年开源 LLM 爆炸式增长（如 Alpaca, Vicuna）的关键催化剂。

### **3.2 指令调优 (Instruction Tuning, SFT)**

#### **3.2.1 SFT 核心概念**

* **定义：** SFT (Supervised Fine-Tuning) 是一种有监督的微调范式，专指“指令调优”（Instruction Tuning）47。  
* **目标：** 预训练模型的目标是“预测下一个词”，而用户的目标是“遵循指令”。SFT 旨在弥合这两个目标之间的差距 48。  
* **实现：** 预训练模型（LLM）在由（指令, 输出）对组成的数据集上进行进一步训练 49。模型被训练以最小化目标序列（输出）的负对数似然（NLL）损失 51。

#### **3.2.2 FLAN (Google, 2021\)**

"Finetuned Language Models Are Zero-Shot Learners" 52 开创了指令调优。

* **核心论点：** 简单地扩大模型规模（如 GPT-3）并不足以让模型很好地进行零样本学习。  
* **实现细节：**  
  * **方法：** 提出“指令调优”（Instruction Tuning）。  
  * **数据：** 在一个包含**超过 60 个 NLP 数据集**（53）的集合上对一个 137B 的预训练模型进行微调。这些数据集被转换（使用模板）成自然语言指令的格式 52。  
* **实验与论证：**  
  * **核心发现：** 指令调优**显著提高**了模型在\*\*未见过（Unseen）**的任务类型上的**零样本（Zero-shot）\*\*性能 52。  
  * **对比：** FLAN（指令调优的 137B 模型）在 25 个评估任务中的 20 个上，其零样本性能*超越*了 175B 的 GPT-3 52。

#### **3.2.3 Alpaca (Stanford, 2023\)**

Alpaca 55 项目展示了 SFT 范式的“数据合成”路径。

* **核心论点：** 展示了如何以极低成本（\< $600）复制出 InstructGPT 级别的指令遵循能力。  
* **实现细节：**  
  * **基础模型：** Meta 的 LLaMA 7B 模型 55。  
  * **数据生成：** 采用了 "Self-Instruct" 57 方法：使用 OpenAI 的 text-davinci-003 55 作为“教师模型”，从 175 个人类编写的种子任务（seed prompts）55 出发，自动生成了 **52,000** 条指令-输出对 55。  
  * **微调：** 在这 52k 合成数据上对 LLaMA 7B 进行 SFT。

#### **3.2.4 LIMA (Meta AI, 2023\)**

"LIMA: Less Is More for Alignment" 58 对 Alpaca 的“数量”路径提出了挑战。

* **核心论点（肤浅对齐假设）：** 提出“肤浅对齐假设”（Superficial Alignment Hypothesis）60。该假设认为：模型的所有知识和能力几乎完全是在**预训练**阶段学到的；“对齐”（Alignment）阶段（如 SFT）只是在教模型应该使用哪种**响应格式**（format）或子分布（sub-distribution）来回答问题 60。  
* **实验设计（LIMA）：**  
  * **基础模型：** LLaMa 65B 58。  
  * **数据：** **故意不**使用 RLHF，也**不**使用大量（如 Alpaca 的 52k）数据。而是由作者精心策划、编写了 **1,000** 个高质量的（提示, 回应）样本 59。  
  * **训练：** 在这 1,000 个样本上使用标准的有监督损失进行微调 61。  
* **实验与论证：**  
  * **结果：** 仅用了 1,000 个样本的 LIMA 表现出了惊人的强大性能，能够泛化到未见过的复杂任务（如规划行程、推测历史）59。  
  * **人工评估：** 在受控的人类研究中，LIMA 的回答在 43% 的情况下被认为与 GPT-4 “相当或更优”；与 Bard 相比为 58%；与 DaVinci003（经过 RLHF 训练）相比为 65% 59。

SFT 数据策略分析：  
SFT 领域存在一个根本性的张力：数据质量 (LIMA) vs 数据数量 (Alpaca/FLAN)。FLAN 53 和 Alpaca 55 遵循“数量/多样性”路径，假设对齐需要大规模、多样化的指令。LIMA 61 则反其道而行之，其“肤浅对齐假设”60 认为 Alpaca 的 52k 数据中可能只有少数高质量样本在起作用。LIMA 的惊人结果（1k 样本 $\\approx$ GPT-4）表明，当前主流的“数据驱动”对齐方法可能是低效的，投资于昂贵的“人类专家标注”可能是通往 SOTA 的捷径。

### **3.3 偏好对齐 (Preference Alignment)**

SFT 教会模型*如何*回答，但不能保证回答是*好的*（例如，有帮助、诚实、无害）。

#### **3.3.1 InstructGPT (RLHF, 2022\)**

"Training language models to follow instructions with human feedback" 62 引入了 RLHF (Reinforcement Learning from Human Feedback)。

* **核心论点：** 更大的模型（如 GPT-3）并不天然更善于遵循用户意图（可能生成不真实、有毒的输出）62。RLHF 是一个将模型与用户意图对齐的有效方法 63。  
* 实现细节（三阶段过程）62：  
  1. **SFT（阶段 1）：** 收集人类演示数据（labeler-written prompts 和 API prompts），对 GPT-3 进行有监督微调（SFT Model）62。  
  2. **奖励模型（RM）训练（阶段 2）：** 收集一个“人类偏好比较”数据集。对于一个 prompt，SFT 模型生成 $K$ 个回答（如 4-9 个），人类标注者对这些回答进行排序 63。训练一个 RM（一个 6B 的 GPT-3 模型）63，输入（prompt, output），输出一个标量“奖励”（reward），以预测人类会给哪个回答更高的排序 62。  
  3. **RL（PPO）优化（阶段 3）：** 将 SFT 模型视为“策略”（Policy），将 RM 视为“奖励函数”。使用\*\*近端策略优化（PPO）\*\*算法 65 调整 SFT 模型的参数，以最大化来自 RM 的预期奖励 62。  
* **实验与论证：**  
  * **核心结果：** **1.3B** (13亿) 参数的 InstructGPT 模型，在人类评估中，其输出被认为*优于* **175B** (1750亿) 参数的 GPT-3 模型 62。  
  * **PPO-ptx：** 为了防止模型在 RL 阶段“遗忘”预训练的知识，他们将 PPO 梯度与预训练分布的梯度进行了混合（PPO-ptx）63。

#### **3.3.2 DPO: Direct Preference Optimization (2023)**

"Direct Preference Optimization: Your Language Model is Secretly a Reward Model" 66 旨在简化 RLHF。

* **核心论点：** RLHF（特别是 PPO）过程复杂、难以实现且训练不稳定 67。DPO 是一种无需强化学习（RL-free）的替代方案 69。  
* **实现细节：**  
  * **绕过 RM：** DPO 巧妙地证明了，RLHF 的奖励最大化目标可以被重新参数化，从而*绕过*显式的奖励模型（RM）训练 69。  
  * **实现：** DPO *直接*使用 SFT 模型（作为参考策略 $\\pi\_{ref}$）和人类偏好数据集（(x, $y\_w$, $y\_l$)，其中 $y\_w$ 是首选答案，$y\_l$ 是被拒绝的答案）72。  
  * **损失函数：** 使用简单的**二元交叉熵目标**（Binary Cross-Entropy Objective）68 来优化模型，使其*增加* $y\_w$ 的概率，*降低* $y\_l$ 的概率，同时通过 $\\pi\_{ref}$ 进行正则化 67。  
* **实验与论证：**  
  * **性能：** DPO 在摘要、对话和情感控制等任务上，其性能与 PPO-based RLHF 相当，甚至更好 66。  
  * **稳定性：** DPO 显著简化了训练过程，计算效率更高，且更稳定 68。

对齐算法分析：  
从 RLHF 62 到 DPO 70 的演变至关重要。RLHF 是一个高门槛、高复杂度的“系统工程”，需要训练两个模型（RM 和 Policy）并依赖不稳定的 PPO 算法。DPO 将这个复杂的“RL 问题”重新表述为一个简单的“分类问题”（偏好 $y\_w$ 还是 $y\_l$）。这种简化 68 使其迅速成为开源社区（如 Llama 3-Instruct）的事实标准，因为它易于实现和训练。

---

**表 2：指令与偏好对齐技术对比**

| 对齐阶段 | 方法 | 核心机制 | 数据需求 | 关键代表 |
| :---- | :---- | :---- | :---- | :---- |
| 阶段 1： 指令调优 | SFT (数量) | 有监督学习 (Instruction, Output) 对。 | 大规模、多样化的指令。 | **FLAN** 53 (60+ NLP 数据集) **Alpaca** 55 (52k 合成数据) |
|  | SFT (质量) | 有监督学习 (高策划) | 极少量、高质量的指令。 | **LIMA** 61 (1,000 策划样本) |
| 阶段 2： 偏好对齐 | RLHF 62 | 1\. 训练 RM (人类偏好排序) 2\. PPO 强化学习 (最大化 RM 奖励) | SFT 数据 \+ 偏好排序数据。 | **InstructGPT** 62 |
|  | DPO 70 | RL-free。直接使用 BC 损失优化偏好 (yw, yl)。 | SFT 数据 \+ 偏好对 (yw, yl)。 | (Mixtral, Llama 3-Instruct 等) 48 |

## **四、 推理、智能体与高级应用**

本章探讨 LLM 的两个前沿方向：(1) 如何优化和理解其*推理（Inference & Reasoning）过程；(2) 如何将其从“文本处理器”转变为智能体（Agent）*。

### **4.1 推理优化 (Inference Optimization)**

#### **4.1.1 KV 缓存与投机解码**

* **KV 缓存 (KV Cache)：** 在自回归解码中，Transformer 需要存储先前所有 Token 的键（K）和值（V）(KV Cache)。随着序列变长（如 34 的 32k），KV 缓存本身成为巨大的内存瓶颈 73。  
* **投机解码 (Speculative Decoding, SD)：**  
  * **动机：** 自回归生成是*串行*的（一次一个 Token），这使得强大的 GPU 核心在等待内存 I/O 时处于空闲状态，导致延迟高 74。  
  * 实现 75： SD 是一种“草稿-验证”（Draft-then-Verify）范式。  
    1. **草稿（Draft）：** 使用一个*小型、快速*的“草稿模型”快速地生成 $\\gamma$ 个（例如 4 个）候选 Token。  
    2. **验证（Verify）：** 使用*原始、大型*的“目标模型”对这 $\\gamma$ 个 Token *并行*（一次前向传播）进行验证。  
    3. **接受/拒绝：** 通过一种拒绝采样逻辑 74 比较草稿和目标模型的概率分布，决定接受（并前进）多少个 Token。  
  * **优势：** 在保持输出分布*完全不变*（Lossless）78 的前提下，实现 2-3 倍的推理加速。

#### **4.1.2 论文："Fast Inference from Transformers via Speculative Decoding" (2023)**

* **核心论点：** 利用一个小型草稿模型（$M\_q$）和一个大型目标模型（$M\_p$）来加速解码，且不改变目标模型的输出分布 79。  
* **实现细节（算法 1）：**  
  1. 草稿模型 $M\_q$ 自回归地生成 $\\gamma$ 个 Token。  
  2. 目标模型 $M\_p$ 并行地（在 $\\gamma+1$ 个 Token上）计算一次。  
  3. 比较 $M\_p$ 和 $M\_q$ 的输出分布，决定接受点（第一个 $M\_p$ 和 $M\_q$ 预测不一致的 Token）。  
  4. 接受所有一致的 Token，并从 $M\_p$ 的分布中采样一个新的 Token（如果所有 $\\gamma$ 都被接受，则采样第 $\\gamma+1$ 个）79。  
* **论证：** 定义“接受率”（Acceptance Rate） $\\beta$ 79 作为 $M\_q$ 逼近 $M\_p$ 的度量。加速比（Speedup）与 $\\gamma$ 和 $\\beta$ 相关。

#### **4.1.3 上下文挑战："Lost in the Middle" (2023)**

"Lost in the Middle: How Language Models Use Long Contexts" 37 对长上下文（Long Context）能力提出了质疑。

* **核心论点：** 尽管模型（如 34 的 Mixtral）的上下文窗口越来越长，但这不代表它们能*有效*利用这些上下文 81。  
* 实验设计 81：  
  1. **多文档问答 (Multi-Document QA)：** 将包含答案的“相关文档”放置在输入上下文的不同位置（从开头到结尾）。  
  2. **键值检索 (Key-Value Retrieval)。**  
* **实验与论证：**  
  * **核心发现（U型曲线）：** 性能在相关信息位于上下文的**最开始**或**最后面**时最高 37。  
  * **性能下降：** 当相关信息被放置在**长上下文的中间**时，模型性能**显著下降** 37。  
* **结论：** 这暴露了当前 LLM（即使是声称支持长上下文的模型）在处理长序列时存在根本性的注意力或检索缺陷 37。这对所有依赖长上下文的应用（如 RAG）都是一个灾难性的警告：当相关信息*恰好*落在中间时，模型可能会**静默地失败（silently fail）**。

### **4.2 LLM 推理能力 (Reasoning in LLMs)**

#### **4.2.1 思维链 (CoT): "Chain-of-Thought Prompting Elicits Reasoning" (2022)**

* **核心论点：** (Wei et al. 2022\) 83。标准的提示（Input-Output）无法让 LLM 解决复杂的（如多步骤）推理任务。CoT 通过引导 LLM 生成*中间推理步骤*来分解问题，从而实现复杂推理 84。  
* **实现细节：**  
  * Few-shot CoT 86： 在提示中提供几个（例如 8 个）完整的（问题, 思维链, 答案）的示例。  
  * Zero-shot CoT 85： (Kojima et al. 2022)。一种更简单的实现，只需在问题末尾添加一个“魔术短语”，如 **"Let's think step by step"** (让我们一步一步思考) 85。  
* **实验与论证：**  
  * **任务：** 算术推理（GSM8K 87）、常识推理、符号推理。  
  * **结果：** CoT 显著提高了 LLM 在这些任务上的性能 88。  
  * **优势：** CoT 使模型能够将多步骤问题分解为中间步骤，并提供了模型行为的“可解释性窗口” 87。

#### **4.2.2 思维树 (ToT): "Tree of Thoughts" (2023)**

* **核心论点：** CoT 是一种*线性*（Linear）的推理过程；一旦某一步出错，整个链条都会失败。ToT 89 将其推广为*树状*（Tree）搜索 90。  
* **实现细节：**  
  * **框架：** ToT 将问题解决定义为在一个树上搜索，其中每个节点 $s \= \[x, z\_{1...i}\]$ 是一个包含部分解决方案的状态 90。  
  * 实现四问 90： (1) 如何分解问题；(2) 如何从当前状态生成多个（$k$ 个）潜在的“思想”（下一步）91；(3) 如何用启发式方法（Heuristic）评估这些思想；(4) 使用何种搜索算法（如 BFS, DFS）来探索这棵树。  
* **实验与论证：**  
  * **任务：** 具挑战性的任务，如 GSM8K 90、StrategyQA 90、Game of 24。  
  * **结果：** 在 CoT 和 CoT-SC (Self-Consistency) 91 失败的任务上，ToT 成功地找到了解决方案。

### **4.3 LLM 智能体 (Agents)**

#### **4.3.1 智能体核心概念**

* **定义：** LLM Agent（智能体）是一种能够感知环境、进行决策和执行动作的智能实体 92。  
* **角色：** LLM 不再是工具，而是充当“中央决策者”或“推理引擎” 94，通过自然语言触发全自动的工作流 92。  
* **编排 (Orchestration)：** (如 LangChain) 94。管理 LLM 与各种工具、API 之间的交互 94。

#### **4.3.2 ReAct: "Synergizing Reasoning and Acting" (2022)**

* **核心论点：** (Yao et al. 2022\) 95。以前的工作要么只“推理”（Reasoning, 如 CoT 96），要么只“行动”（Acting, 如 Act-only）。ReAct 论证了*结合*这两者的协同作用（Synergy）是实现智能的关键 96。  
* **实现细节：**  
  * **ReAct 框架：** 提出了一种“思想-行动-观察”（Thought-Action-Observation）的循环 97。  
  * 循环 96：  
    1. **Thought (思想)：** LLM (CoT) 进行推理，决定*下一步做什么*。  
    2. **Action (行动)：** LLM 决定调用一个“工具”（如 Search\[Query\] 或 LookUp）。  
    3. **Observation (观察)：** 从“环境”（如 API）返回结果（如搜索摘要）。  
  * **反馈循环：** LLM 接收这个 Observation，并开始下一个 Thought，形成一个反馈循环 97。  
* **实验与论证：**  
  * **任务：** 知识密集型任务 (HotpotQA) 和决策密集型任务 (AlfWorld, WebShop) 98。  
  * **对比：** ReAct（Reason+Act）显著优于 CoT（Reason-only）和 Act-only 基线 96。  
  * **协同作用：** ReAct 展示了“为行动而推理”（reason to act，规划）和“为推理而行动”（act to reason，获取信息）的能力 96。

#### **4.3.3 Toolformer: "Language Models Can Teach Themselves to Use Tools" (2023)**

* **核心论点：** LLM 无法执行算术或访问实时信息 99。我们可以训练 LLM *学会*使用外部工具（API）来弥补这些缺陷 101。  
* 实现细节（自监督）100： 这是一个巧妙的*自监督*数据生成过程：  
  1. **采样：** 让 LLM (GPT-J) 99 针对几个“工具使用”的手写示例，为*新的*文本采样*潜在的* API 调用。  
  2. **执行：** *执行*这些 API 调用（例如，调用计算器或搜索引擎）。  
  3. **过滤：** 比较 API 返回的结果与 LLM *原始*的（无工具的）后续文本。如果 API 结果*有助于*（例如，提高了）LLM 预测后续文本的概率，则*保留*这个（文本, API调用, 结果）三元组。  
  4. **微调：** 将这些“好的”三元组作为 SFT 数据，微调 LLM 102。  
* **实验与论证：**  
  * **结果：** Toolformer 在下游任务（如数学、问答）上的零样本性能显著提高，甚至优于大 10-25 倍的 GPT-3 99。

#### **4.3.4 Generative Agents: "Interactive Simulacra of Human Behavior" (2023)**

* **核心论点：** 融合 LLM 和交互式智能体，以模拟可信的（Believable）人类行为 103。  
* **实现细节（架构）：**  
  1. **记忆流 (Memory Stream)：** 105。一个包含智能体所有经验（观察）的完整记录（使用自然语言）。  
  2. **反思 (Reflection)：** 106。智能体*周期性地*（例如，当事件重要性分数总和超过阈值时）暂停，并对过去的记忆进行“更高层次、更抽象”的综合与反思 106。  
  3. **规划 (Planning)：** 105。基于反思和记忆，动态地规划未来的行动。  
* **实验与论证：**  
  * **环境：** 在一个受《模拟人生》（The Sims）启发 107 的沙盒环境“Smallville”中，实例化了 25 个智能体 107。  
  * **涌现行为（Emergent Behavior）：** 在两天的模拟中，智能体表现出可信的个人行为和*涌现*的社会行为，如：信息扩散、关系形成、和集体协调（如自发地组织一个派对）107。

智能体演进分析：  
本章清晰地勾勒了智能体（Agent）的三个进化阶段：

1. **被动推理（Passive Reasoning）：** CoT 84 和 ToT 89 在 LLM 内部“封闭世界”中运行。  
2. **反应式代理（Reactive Agents）：** ReAct 96 和 Toolformer 101 将 LLM 与*外部工具*连接起来，以响应用户查询。  
3. **自主代理（Autonomous Agents）：** Generative Agents 103 拥有*内部动机*。其“反思”（Reflection）机制 106 是从层次 2 跳到层次 3 的*关键组件*，它为智能体提供了“内在声音”和“长期目标”，使其摆脱了对外部提示的依赖。

---

**表 3：LLM 推理与智能体框架对比**

| 框架 | 核心思想 | 实现机制 | 解决的问题 | 关键实验 |
| :---- | :---- | :---- | :---- | :---- |
| **CoT** 84 | *被动推理*：引导 LLM 分解步骤。 | 提示工程 (Few-shot 或 Zero-shot "Let's think...") | 复杂推理（如数学）。 | GSM8K 87 |
| **ToT** 90 | *被动推理*：将推理视为树搜索。 | 提示工程 (Generate, Evaluate, Search) | CoT 失败的复杂推理。 | Game of 24 90 |
| **ReAct** 96 | *反应式代理*：协同推理和行动。 | Thought-Action-Observation 循环。 | 事实性 (幻觉) 和环境交互。 | HotpotQA, AlfWorld 96 |
| **Toolformer** 101 | *反应式代理*：教会 LLM 自主使用工具。 | 自监督数据生成 (采样, 执行, 过滤) \+ SFT。 | LLM 的内在能力缺陷（如算术）。 | 优于 GPT-3 99 |
| **Gen. Agents** 103 | *自主代理*：模拟可信的人类行为。 | Memory Stream \+ Reflection \+ Planning 架构。 | 实现 LLM 的自主性和长期目标。 | Smallville 模拟 107 |

## **五、 领域应用：多模态与代码**

本章探讨 LLM 如何赋能两个关键的垂直领域：AIGC（图像生成）、多模态（图文理解）和代码（软件工程）。

### **5.1 AIGC 与扩散模型 (Diffusion)**

* **AIGC：** “人工智能生成内容”（AI Generated Content）的缩写 108。利用 AI 技术生成文本、图像、音频等 109。  
* **扩散模型 (Diffusion Models)：** AIGC（特别是图像）的核心技术 109。

#### **5.1.1 DDPM: "Denoising Diffusion Probabilistic Models" (2020)**

* **核心论点：** (Ho et al. 2020\) 110。一种新的生成模型范式，通过逐步“去噪”来生成高质量样本 110。  
* **实现细节：**  
  1. **前向过程（加噪）：** 一个固定的马尔可夫过程（Markov Chain），逐步（例如 $T=1000$ 步）向原始图像 $x\_0$ 添加高斯噪声，直至其变为纯噪声 $x\_T$ 111。  
  2. **反向过程（去噪）：** 训练一个深度神经网络（通常是 U-Net），使其在每一步 $t$ 预测出添加的噪声（或 $x\_{t-1}$）112。  
  3. **生成：** 从纯噪声 $x\_T$ 开始，迭代地使用训练好的神经网络进行 $T$ 次“去噪”，最终得到生成图像 $x\_0$ 112。

#### **5.1.2 LDM: "High-Resolution Image Synthesis with Latent Diffusion Models" (2022)**

* **核心论点：** DDPM 111 功能强大，但在*像素空间*（Pixel Space）中运行，计算成本极高（因为 U-Net 必须在完整分辨率下操作）113。  
* 实现细节（两阶段训练）114：  
  1. **感知压缩（阶段 1）：** 首先训练一个*自动编码器*（Autoencoder, VAE）。编码器 $\\mathcal{E}$ 将高维图像 $x$ 压缩到一个低维、感知上等效的**潜在空间（Latent Space）** $z \= \\mathcal{E}(x)$。  
  2. **潜在扩散（阶段 2）：** *在低维的潜在空间 $z$ 中*训练一个 DDPM 模型，而不是在 $x$ 上。  
* **实验与论证：**  
  * **优势：** (1) 极大降低了训练和推理的计算需求 113；(2) VAE 编码器（$\\mathcal{E}$）丢弃了高频、非语义的“噪音”，使扩散模型能更专注于数据的*语义*和*概念*部分 114。

#### **5.1.3 ControlNet: "Adding Conditional Control to Text-to-Image Diffusion Models" (2023)**

* **核心论点：** 像 Stable Diffusion (LDM) 116 这样的大型预训练模型，很难通过简单的 Prompt（提示）来实现精确的*空间*控制（如姿势、边缘、深度图）117。  
* 实现细节（架构）116：  
  1. **锁定（Lock）：** 将预训练的扩散模型（如 SD U-Net）的参数*完全锁定*，将其视为一个强大的、不可变的主干 116。  
  2. **复制（Copy）：** 创建一个该模型编码层（Encoder）的*可训练副本*（Trainable Copy）116。  
  3. **零卷积（Zero Convolutions）：** 119。使用 1x1、权重和偏置初始化为*零*的卷积层，将“可训练副本”的输出添加到“锁定主干”的对应层 119。  
* **实验与论证：**  
  * **优势：** “零卷积” 119 保证了在训练开始时，ControlNet 不会对主干模型产生任何*有害*的噪声（因为添加的是 0），使得训练非常稳定。  
  * **数据鲁棒性：** 实验证明 ControlNet 在小型（\<50k）和大型（\>1m）数据集上训练都非常鲁棒 117。

### **5.2 多模态大型模型 (MLLMs)**

* **定义：** MLLM 结合了 LLM（文本）与视觉、音频等其他模态 120。  
* 典型架构 121： (1) 预训练的**模态编码器**（如 CLIP）；(2) 一个**模态接口**（Connector）；(3) 一个预训练的 **LLM 骨干**（如 LLaMA）。

#### **5.2.1 CLIP: "Learning Transferable Visual Models From Natural Language Supervision" (2021)**

* **核心论点：** (OpenAI, 2021\) 122。通过从互联网收集的 4 亿（400M）个（图像, 文本）对，以*自然语言监督*的方式，训练出一个强大的、可迁移的视觉模型 123。  
* **实现细节（对比学习）：**  
  * **架构：** 包含一个图像编码器（Image Encoder, ViT 124）和一个文本编码器（Text Encoder, Transformer）123。  
  * **训练：** 采用*对比学习*（Contrastive Learning）。在一个 Batch 中，模型的目标是最大化*匹配*的（图像, 文本）对的余弦相似度，同时最小化*不匹配*的对的相似度 125。  
* **实验与论证：**  
  * 核心能力：零样本迁移（Zero-Shot Transfer）126： CLIP 的革命性贡献。它可以在*未见过*的数据集（如 ImageNet）上实现 SOTA 性能，*无需*任何微调。  
  * **实现：** 将分类任务转换为“图文匹配”。例如，对于 ImageNet，创建 1000 个 Prompt（如 "a photo of a *\[label\]*"），模型选择与图像嵌入最相似的 Prompt 126。

#### **5.2.2 LLaVA: "Visual Instruction Tuning" (2023)**

* **核心论点：** (Liu et al. 2023\) 127。将 LLM 的“指令调优”（Instruction Tuning）能力扩展到多模态领域 127。  
* 实现细节（架构）128： LLaVA 129 完美遵循了 MLLM 的典型架构：  
  1. **视觉编码器：** *预训练*的 CLIP ViT-L/14 128（冻结）。  
  2. **LLM 骨干：** *预训练*的 Vicuna 128（一个 LLaMA 的指令微调版）。  
  3. **接口：** 一个*简单*的线性投影矩阵（Projection Matrix）128。  
* 实验与论证（两阶段训练）128：  
  1. **阶段 1（特征对齐）：** *冻结* CLIP 和 Vicuna，*只*训练投影矩阵。使用 CC3M 的一个子集，目标是让视觉特征对齐到 LLM 的词嵌入空间。  
  2. **阶段 2（端到端微调）：** *冻结* CLIP，*解冻*投影矩阵和 LLM。在一个（合成的）多模态指令遵循数据集（Visual Chat）或 Science QA 127 上进行端到端微调。  
* **结果：** LLaVA 展示了令人印象深刻的多模态聊天能力，在合成的指令遵循数据集上，相对 GPT-4 取得了 85.1% 的分数 127。

应用范式分析：  
本章的 SOTA 应用（LDM 114, ControlNet 116, LLaVA 128）均非“从头构建”，而是体现了\*\*“组合式 AI”（Composable AI）\*\*的范式。LLaVA 的成功尤其具有启发性：它（1）依赖 CLIP 123 作为“隐形基石”，因为 CLIP 已经完成了 99% 的跨模态对齐工作；（2）使用一个极其简单的线性层 128 就将两个强大的、独立的预训练模型（CLIP 和 Vicuna）“胶合”在了一起。

### **5.3 代码生成 (Code Generation)**

* **AI4Code：** 指将 AI（特别是 LLM）应用于源代码的任务 130，如文本到代码（Text-to-Code）132、代码补全 130 和代码分析 133。

#### **5.3.1 综述："A Survey of Large Language Models for Code" (2023)**

* **核心论点：** 对 Code LLM 的演进、基准和趋势进行全面调研 134。  
* **发现：**  
  * **分类：** 收集了 134 篇作品 134，并根据发布者对 Code LLM 进行了分类 135。  
  * **性能：** Code LLM（专门在代码上训练/微调）在软件工程任务上显著优于 General LLM（通用 LLM）134。  
  * **局限：** LLM 在静态分析（Static Analysis）任务上表现不佳 136。

#### **5.3.2 WizardCoder: "Empowering Code Large Language Models with Evol-Instruct" (2023)**

* **核心论点：** Code LLM（如 StarCoder）的性能受限于*指令的简单性*。本文提出 Code Evol-Instruct，一种将 Alpaca 的 "Evol-Instruct" 方法应用于代码领域的新方法 137。  
* **实现细节（Code Evol-Instruct）：**  
  * **动机：** Alpaca 55 的指令过于简单。  
  * **方法：** 使用 LLM（如 GPT-4）对*现有的*代码指令进行“进化”（Evolve），使其变得更复杂 137。  
  * 进化策略 138： (1) 增加更具体的（冷门）需求；(2) 增加更多的逻辑推理步骤；(3) 提供错误代码作为“干扰项”（Misdirection）；(4) 提出更高的时空复杂度要求。  
* **实验与论证：**  
  * **模型：** WizardCoder (StarCoder base \+ Evol-Instruct SFT) 137。  
  * **基准：** HumanEval, HumanEval+, MBPP, DS-1000 137。  
  * **结果：** WizardCoder（15B 和 34B）的性能**全面超越**了所有现有的开源 Code LLM，甚至超越了*闭源*的 Claude 和 Bard 139。  
  * 消融研究 141： 实验了不同“进化轮次”（evolution rounds）的数据。发现*第 3 轮*进化的数据（78k）在 HumanEval 上 Pass@1 得分最高。第 4 轮（98k）性能反而下降，表明“进化”并非越复杂越好 138。

## **六、 关键基础设施**

本章深入探讨使上述所有模型（从训练到推理）成为可能的底层技术。

### **6.1 AI 基础设施核心概念**

* **动机：** 大型模型的训练和推理受到硬件（主要是 GPU 显存容量和内存带宽）的严重限制。  
* **关键技术：**  
  * ZeRO 142： 针对*训练*时的*显存*优化。  
  * FlashAttention 142： 针对*训练和推理*时的*内存带宽*和*显存*优化。  
  * 量化 (Quantization) 142： 降低模型大小和内存占用的通用技术（见 3.1.3 QLoRA）。

### **6.2 FlashAttention: "Fast and Memory-Efficient Exact Attention with IO-Awareness" (2022)**

* **核心论点（瓶颈分析）：** (Dao et al. 2022\) 144。标准注意力 1 的瓶颈*不是* FLOPs（计算），而是**内存访问（HBM Access）** 145。它是一个\*\*“内存带宽受限”（Memory-Bound）\*\*的操作 145。  
* **问题：** 标准注意力实现中，计算 $S \= QK^T$ 并计算 $P \= \\text{Softmax}(S)$，需要一个 $N \\times N$ 的矩阵（$N$ 是序列长度）。这个 $N \\times N$ 矩阵 $S$ 必须被*写入*到 HBM（高带宽内存，即显存），然后再被*读回* HBM 以计算 $P$ 145。  
* **实现细节（IO-Awareness）：** FlashAttention 的目标是*绝不*将完整的 $N \\times N$ 矩阵 $S$ 写入 HBM 146。  
  1. 分块/平铺（Tiling）145： 将 $Q, K, V$ 分成块（Blocks）。一次只从 HBM 加载 $Q, K, V$ 的一个块到 GPU 的*超快* SRAM（片上内存）146。  
  2. 内核融合（Kernel Fusion）146： 在 SRAM 内部，用*一个* CUDA 核（Kernel）完成*所有*计算（矩阵乘法、Softmax、矩阵乘法），然后才将*最终*的输出块写回 HBM。  
  3. **重计算（Recomputation）：** 在反向传播（Backward pass）时，*不*存储中间值 $S$ 和 $P$（因为它们从未写入 HBM），而是利用存储在 SRAM 中的块*重新计算*它们。这是一种典型的以计算换内存（带宽）的策略 146。  
* **实验与论证：**  
  * **内存：** 将注意力对序列长度 $N$ 的内存占用从 $O(N^2)$ 降低到 $O(N)$（线性）147。  
  * **速度：** 比标准 PyTorch 实现快 3 倍 147。  
  * **影响：** 首次使 Transformer 能够在超长序列（如 Path-X, 16K）上实现端到端训练 147。

### **6.3 ZeRO: "Memory Optimizations Toward Training Trillion Parameter Models" (2019)**

* **核心论点：** (Rajbhandari et al. 2019\) 148。现有的数据并行（Data Parallelism, DP）在每个 GPU 上复制完整的模型状态（参数、梯度、优化器状态），导致巨大的内存冗余 148。  
* **实现细节：** ZeRO (Zero Redundancy Optimizer) 是一种增强的数据并行，它通过在数据并行的 GPU 之间\*\*分区（Partitioning）\*\*模型状态来消除内存冗余 148。  
* ZeRO-DP 的三个阶段 150：  
  * **Pos 1 (阶段 1)：优化器状态分区（Optimizer State Partitioning）。** 优化器状态（如 Adam 的动量）被分区到 $N\_{DP}$ 个 GPU 上（每个 GPU 只保留 $1/N\_{DP}$）。  
  * **Pos 2 (阶段 2)：梯度分区（Gradient Partitioning）。** 梯度也被分区。  
  * **Pos 3 (阶段 3)：参数分区（Parameter Partitioning）。** 模型参数（$W$）也被分区。  
* **实验与论证：**  
  * **结果：** ZeRO 能够训练 100B+（1000亿）参数的模型 148，并在 400 个 GPU 上实现 15 Petaflops 的吞吐量，比 SOTA 提高了 10 倍 151。  
  * **可用性：** ZeRO-DP Stage 1+2 可以在不改变模型代码的情况下训练高达 13B 的模型（无需模型并行）151。

基础设施影响分析：  
ZeRO 148 和 FlashAttention 147 解决了两个正交（Orthogonal）且互补的问题。ZeRO 解决了训练时模型状态的冗余；FlashAttention 解决了训练和推理时注意力矩阵的 I/O 瓶颈。  
这种基础设施的进步是AI研究的元驱动力。在 FlashAttention 出现之前，研究 $N \> 4k$ 的序列是极其昂贵的。FlashAttention 147 使得在 16k+ 序列上训练变得可行，这反过来才使得研究人员能够进行 "Lost in the Middle" 37 的受控实验。这揭示了一个研究的元模式：基础设施的进步（Infra） $\\rightarrow$ 启用新的模型能力（Model） $\\rightarrow$ 暴露新的、更深层次的失败模式（Problem） $\\rightarrow$ 催生新的研究（Research）。

---

**表 4：训练与推理基础设施优化技术**

| 技术 | ZeRO | FlashAttention | Speculative Decoding |
| :---- | :---- | :---- | :---- |
| **应用阶段** | **训练 (Training)** | **训练 & 推理** | **推理 (Inference)** |
| **解决问题** | **模型状态 (State)** 的显存冗余。 | **注意力矩阵 ($S$)** 的内存带宽 (I/O) 瓶颈。 | **自回归生成** 的串行延迟 (Latency) 瓶颈。 |
| **主要敌人** | 数据并行 (DP) 导致的冗余。 | $O(N^2)$ 的 HBM 读写（$N$=序列长）。 | 内存带宽受限的串行 Token 生成。 |
| **核心机制** | **分区 (Partitioning)** (Pos 1-3) | **Tiling \+ Kernel Fusion \+ Recomputation** | **Draft-then-Verify** (小模型草稿 \+ 大模型并行验证) |
| **对 $N$ 的影响** | 随 $N\_{params}$ 线性扩展。 | 使内存随 $N\_{seq}$ 线性 ($O(N)$) 扩展。 | 加速与 $N\_{seq}$ 无关，与 $\\gamma$ (草稿长度) 相关。 |
| **最终效果** | 训练万亿参数模型成为可能 148。 | 训练/推理超长上下文 (16k+) 成为可能 147。 | 推理速度提升 2-3x，且输出无损 78。 |

## **七、 总结与趋势**

本报告对大型语言模型（LLM）的关键论文和技术进行了系统性梳理，揭示了该领域发展的几大核心趋势：

1. **架构的统一（Decoder-Only）与抽象（DPO）：** 领域正从 BERT 6 和 GPT 12 的架构分裂走向解码器-Only 架构的统一。同时，复杂的对齐算法（如 RLHF 62）正被更简单、更数学抽象的算法（如 DPO 70）所取代，这极大地推动了开源社区的发展。  
2. **组合式 AI（Composable AI）的兴起：** SOTA 应用（如 LLaVA 128, LDM 114, ControlNet 116）的实现，不再依赖于“从头训练”，而是转向将多个强大的、预训练的组件（如 CLIP 123, LLaMA）通过微小的“适配器”巧妙地“胶合”在一起。  
3. **数据瓶颈的转向（Quantity vs. Quality）：** Chinchilla 26 终结了“模型优先”的缩放法则，将行业的瓶颈转向“数据量”（Quantity）。而 LIMA 61 则进一步挑战了这一点，其“肤浅对齐假设”60 表明，对齐阶段的“数据质量”（Quality）可能远比数量更重要。这一“质量 vs 数量”的张力是当前 LLM 发展的核心矛盾。  
4. **从被动推理到自主智能体：** LLM 的能力正在经历三个进化阶段：(1) CoT/ToT 84 的“被动推理”；(2) ReAct/Toolformer 96 的“反应式代理”，即 LLM 开始与外部世界（工具）交互；(3) Generative Agents 103 的“自主代理”，即通过“反思”（Reflection）机制 106 产生内部动机和长期目标。  
5. **基础设施驱动的研究范式：** 底层基础设施（如 FlashAttention 147 和 ZeRO 148）不仅是实现的工具，更是研究的驱动力。FlashAttention 使得长上下文研究成为可能，从而*揭示*了 "Lost in the Middle" 37 这一新的、更深层次的失败模式，形成了“基础设施 $\\rightarrow$ 新能力 $\\rightarrow$ 新问题 $\\rightarrow$ 新研究”的闭环。

#### **引用的著作**

1. Attention is All you Need \- NIPS papers, 访问时间为 十一月 5, 2025， [https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf](https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf)  
2. \[1706.03762\] Attention Is All You Need \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)  
3. “Attention is All You Need” Summary \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@dminhk/attention-is-all-you-need-summary-6f0437e63a91](https://medium.com/@dminhk/attention-is-all-you-need-summary-6f0437e63a91)  
4. Attention is all you need: Discovering the Transformer paper | Towards Data Science, 访问时间为 十一月 5, 2025， [https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634/](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634/)  
5. Reviews: Attention is All you Need \- NIPS papers, 访问时间为 十一月 5, 2025， [https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Reviews.html](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Reviews.html)  
6. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \- ACL Anthology, 访问时间为 十一月 5, 2025， [https://aclanthology.org/N19-1423.pdf](https://aclanthology.org/N19-1423.pdf)  
7. \[1810.04805\] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)  
8. Paper summary — BERT: Bidirectional Transformers for Language Understanding | by Sanna Persson | Analytics Vidhya | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9](https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9)  
9. A Complete Guide to BERT with Code | Towards Data Science, 访问时间为 十一月 5, 2025， [https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)  
10. BERT (language model) \- Wikipedia, 访问时间为 十一月 5, 2025， [https://en.wikipedia.org/wiki/BERT\_(language\_model)](https://en.wikipedia.org/wiki/BERT_\(language_model\))  
11. Language Models are Few-Shot Learners \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165)  
12. In-Context Learning, In Context \- The Gradient, 访问时间为 十一月 5, 2025， [https://thegradient.pub/in-context-learning-in-context/](https://thegradient.pub/in-context-learning-in-context/)  
13. \[D\] Paper Explained \- GPT-3: Language Models are Few-Shot Learners (Video Analysis), 访问时间为 十一月 5, 2025， [https://www.reddit.com/r/MachineLearning/comments/gsuzey/d\_paper\_explained\_gpt3\_language\_models\_are/](https://www.reddit.com/r/MachineLearning/comments/gsuzey/d_paper_explained_gpt3_language_models_are/)  
14. Language Models are Few-Shot Learners \- NIPS papers, 访问时间为 十一月 5, 2025， [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)  
15. \[2407.21783\] The Llama 3 Herd of Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783)  
16. The Llama 3 Herd of Models. Paper Review | by Eleventh Hour Enthusiast | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@EleventhHourEnthusiast/the-llama-3-herd-of-models-2f62252ce1c8](https://medium.com/@EleventhHourEnthusiast/the-llama-3-herd-of-models-2f62252ce1c8)  
17. LLaMA 3 vs. State-of-the-Art Large Language Models: Performance in Detecting Nuanced Fake News \- MDPI, 访问时间为 十一月 5, 2025， [https://www.mdpi.com/2073-431X/13/11/292](https://www.mdpi.com/2073-431X/13/11/292)  
18. LoRA: Low-Rank Adaptation of Large Language Models \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9)  
19. 概念- 微调AI 和机器学习工作流的语言模型- Azure Kubernetes Service | Microsoft Learn, 访问时间为 十一月 5, 2025， [https://learn.microsoft.com/zh-cn/azure/aks/concepts-fine-tune-language-models](https://learn.microsoft.com/zh-cn/azure/aks/concepts-fine-tune-language-models)  
20. 什么是参数高效微调(PEFT)？ \- IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/cn-zh/think/topics/parameter-efficient-fine-tuning](https://www.ibm.com/cn-zh/think/topics/parameter-efficient-fine-tuning)  
21. Breaking Down Meta's Llama 3 Herd of Models \- Arize AI, 访问时间为 十一月 5, 2025， [https://arize.com/blog/breaking-down-meta-llama-3/](https://arize.com/blog/breaking-down-meta-llama-3/)  
22. Scaling Laws for Neural Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2001.08361](https://arxiv.org/pdf/2001.08361)  
23. \[2001.08361\] Scaling Laws for Neural Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)  
24. Neural scaling law \- Wikipedia, 访问时间为 十一月 5, 2025， [https://en.wikipedia.org/wiki/Neural\_scaling\_law](https://en.wikipedia.org/wiki/Neural_scaling_law)  
25. OpenAI's research paper “Scaling Laws for Neural Language Models” — Key takeaways, 访问时间为 十一月 5, 2025， [https://medium.com/@lukelimdy/openais-research-paper-scaling-laws-for-neural-language-models-key-takeaways-d177438553d8](https://medium.com/@lukelimdy/openais-research-paper-scaling-laws-for-neural-language-models-key-takeaways-d177438553d8)  
26. Training Compute-Optimal Large Language Models, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)  
27. "Training Compute-Optimal Large Language Models", Hoffmann et al 2022 {DeepMind} (current LLMs are significantly undertrained) : r/mlscaling \- Reddit, 访问时间为 十一月 5, 2025， [https://www.reddit.com/r/mlscaling/comments/trwkck/training\_computeoptimal\_large\_language\_models/](https://www.reddit.com/r/mlscaling/comments/trwkck/training_computeoptimal_large_language_models/)  
28. Chinchilla : A Guide To Training Compute Optimal Large Language Models \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@pranjalkhadka/chinchilla-a-guide-to-training-compute-optimal-large-language-models-3d3ad3787f1d](https://medium.com/@pranjalkhadka/chinchilla-a-guide-to-training-compute-optimal-large-language-models-3d3ad3787f1d)  
29. 访问时间为 十一月 5, 2025， [https://zh.wikipedia.org/wiki/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B\#:\~:text=%E6%B7%B7%E5%90%88%E5%B0%88%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Amixture,%E5%80%8B%E5%AD%90%E6%A8%A1%E5%9E%8B%E4%BE%86%E8%99%95%E7%90%86%E5%95%8F%E9%A1%8C%E3%80%82](https://zh.wikipedia.org/wiki/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B#:~:text=%E6%B7%B7%E5%90%88%E5%B0%88%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88%E8%8B%B1%E8%AA%9E%EF%BC%9Amixture,%E5%80%8B%E5%AD%90%E6%A8%A1%E5%9E%8B%E4%BE%86%E8%99%95%E7%90%86%E5%95%8F%E9%A1%8C%E3%80%82)  
30. Mixture of Experts Explained \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/blog/moe](https://huggingface.co/blog/moe)  
31. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)  
32. Review — Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity \- Sik-Ho Tsang, 访问时间为 十一月 5, 2025， [https://sh-tsang.medium.com/brief-review-switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-880edbbf4890](https://sh-tsang.medium.com/brief-review-switch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-880edbbf4890)  
33. arXiv:2101.03961v3 \[cs.LG\] 16 Jun 2022, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2101.03961](https://arxiv.org/pdf/2101.03961)  
34. Mixtral of experts \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2401.04088](https://arxiv.org/pdf/2401.04088)  
35. Paper page \- Mixtral of Experts \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/papers/2401.04088](https://huggingface.co/papers/2401.04088)  
36. What is mixture of experts? | IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/mixture-of-experts](https://www.ibm.com/think/topics/mixture-of-experts)  
37. Lost in the Middle: How Language Models Use Long Contexts \- MIT Press Direct, 访问时间为 十一月 5, 2025， [https://direct.mit.edu/tacl/article/doi/10.1162/tacl\_a\_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)  
38. Emergent abilities and grokking: Fundamental, Mirage, or both? \- Windows On Theory, 访问时间为 十一月 5, 2025， [https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/](https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/)  
39. I. From GPT-4 to AGI: Counting the OOMs \- SITUATIONAL AWARENESS, 访问时间为 十一月 5, 2025， [https://situational-awareness.ai/from-gpt-4-to-agi/](https://situational-awareness.ai/from-gpt-4-to-agi/)  
40. \[2106.09685\] LoRA: Low-Rank Adaptation of Large Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)  
41. What is LoRA (Low-Rank Adaption)? \- IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/lora](https://www.ibm.com/think/topics/lora)  
42. Low-rank Adaptation of Large Language Models—Implementation Guide \- Nexla, 访问时间为 十一月 5, 2025， [https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/](https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/)  
43. \[2305.14314\] QLoRA: Efficient Finetuning of Quantized LLMs \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314)  
44. artidoro/qlora \- Efficient Finetuning of Quantized LLMs \- GitHub, 访问时间为 十一月 5, 2025， [https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)  
45. QLoRA: Efficient Finetuning of Quantized LLMs \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=OUIFPHEgJU\&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D\~Ari\_Holtzman1)](https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe+profile+of+Ari+Holtzman%5D\(/profile?id%3D~Ari_Holtzman1\))  
46. QLORA: Efficient Finetuning of Quantized LLMs — Paper Review | by Sulbha Jain | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@sulbha.jindal/qlora-efficient-finetuning-of-quantized-llms-paper-re-0984016c9bc9](https://medium.com/@sulbha.jindal/qlora-efficient-finetuning-of-quantized-llms-paper-re-0984016c9bc9)  
47. Instruction Tuning in LLMs \- Emergent Mind, 访问时间为 十一月 5, 2025， [https://www.emergentmind.com/topics/instruction-tuning-774bf361-6e1f-463b-a247-eb83c50b932d](https://www.emergentmind.com/topics/instruction-tuning-774bf361-6e1f-463b-a247-eb83c50b932d)  
48. Supervised Fine-Tuning Explained: Advanced LLM Training Techniques (October 2025), 访问时间为 十一月 5, 2025， [https://www.thundercompute.com/blog/supervised-fine-tuning-guide](https://www.thundercompute.com/blog/supervised-fine-tuning-guide)  
49. Instruction Tuning for Large Language Models: A Survey \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2308.10792v5](https://arxiv.org/html/2308.10792v5)  
50. What Is Instruction Tuning? | IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/instruction-tuning](https://www.ibm.com/think/topics/instruction-tuning)  
51. SFT Trainer \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/docs/trl/sft\_trainer](https://huggingface.co/docs/trl/sft_trainer)  
52. \[2109.01652\] Finetuned Language Models Are Zero-Shot Learners \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2109.01652](https://arxiv.org/abs/2109.01652)  
53. Finetuned Language Models are Zero-Shot Learners \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR)  
54. Review: Fine-tuned language models are zero-shot learners \- Paperspace Blog, 访问时间为 十一月 5, 2025， [https://blog.paperspace.com/instruction-tuning/](https://blog.paperspace.com/instruction-tuning/)  
55. Self-Instruct Style Data Generation: The Secret Behind Stanford Alpaca | by Okan Yenigün, 访问时间为 十一月 5, 2025， [https://medium.com/@okanyenigun/self-instruct-style-data-generation-the-secret-behind-stanford-alpaca-e1575ea9ad71](https://medium.com/@okanyenigun/self-instruct-style-data-generation-the-secret-behind-stanford-alpaca-e1575ea9ad71)  
56. llama-adapter: efficient fine-tuning of large language mod \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2303.16199](https://arxiv.org/pdf/2303.16199)  
57. Alpaca: A Strong, Replicable Instruction-Following Model \- Stanford CRFM, 访问时间为 十一月 5, 2025， [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)  
58. LIMA: Less Is More for Alignment \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/pdf?id=KBMOKmX2he](https://openreview.net/pdf?id=KBMOKmX2he)  
59. \[2305.11206\] LIMA: Less Is More for Alignment \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206)  
60. LIMA: Less Is More for Alignment \- Paper Reading and Discussion \- Arize AI, 访问时间为 十一月 5, 2025， [https://arize.com/blog/lima-less-is-more-for-alignment-paper-reading-and-discussion/](https://arize.com/blog/lima-less-is-more-for-alignment-paper-reading-and-discussion/)  
61. Paper page \- LIMA: Less Is More for Alignment \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/papers/2305.11206](https://huggingface.co/papers/2305.11206)  
62. Training language models to follow instructions with human feedback \- OpenAI, 访问时间为 十一月 5, 2025， [https://cdn.openai.com/papers/Training\_language\_models\_to\_follow\_instructions\_with\_human\_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)  
63. Training language models to follow instructions with human feedback, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)  
64. RLHF: Reinforcement Learning from Human Feedback \- Chip Huyen, 访问时间为 十一月 5, 2025， [https://huyenchip.com/2023/05/02/rlhf.html](https://huyenchip.com/2023/05/02/rlhf.html)  
65. Training language models to follow instructions with human feedback, 访问时间为 十一月 5, 2025， [https://proceedings.neurips.cc/paper\_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)  
66. Direct Preference Optimization: Your Language Model is Secretly a Reward Model, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=HPuSIXJaa9](https://openreview.net/forum?id=HPuSIXJaa9)  
67. Direct Preference Optimization (DPO): a lightweight counterpart to RLHF \- Toloka AI, 访问时间为 十一月 5, 2025， [https://toloka.ai/blog/direct-preference-optimization/](https://toloka.ai/blog/direct-preference-optimization/)  
68. What is direct preference optimization (DPO)? \- SuperAnnotate, 访问时间为 十一月 5, 2025， [https://www.superannotate.com/blog/direct-preference-optimization-dpo](https://www.superannotate.com/blog/direct-preference-optimization-dpo)  
69. 访问时间为 十一月 5, 2025， [https://cameronrwolfe.substack.com/p/direct-preference-optimization\#:\~:text=Direct%20Preference%20Optimization%20(DPO)%20is,(and%20a%20reference%20policy).](https://cameronrwolfe.substack.com/p/direct-preference-optimization#:~:text=Direct%20Preference%20Optimization%20\(DPO\)%20is,\(and%20a%20reference%20policy\).)  
70. Direct Preference Optimization: Your Language Model is Secretly a Reward Model \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2305.18290](https://arxiv.org/pdf/2305.18290)  
71. RLHF vs. DPO: Choosing the Method for LLMs Alignment Tuning | by Baicen Xiao \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@baicenxiao/rlhf-vs-dpo-choosing-the-method-for-llm-alignment-tuning-66f45ef3d4b5](https://medium.com/@baicenxiao/rlhf-vs-dpo-choosing-the-method-for-llm-alignment-tuning-66f45ef3d4b5)  
72. Preference Tuning LLMs with Direct Preference Optimization Methods \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/blog/pref-tuning](https://huggingface.co/blog/pref-tuning)  
73. Closer Look at Efficient Inference Methods: A Survey of Speculative Decoding \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2411.13157v1](https://arxiv.org/html/2411.13157v1)  
74. An Introduction to Speculative Decoding for Reducing Latency in AI Inference, 访问时间为 十一月 5, 2025， [https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/](https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/)  
75. 访问时间为 十一月 5, 2025， [https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/\#:\~:text=Speculative%20decoding%20is%20an%20inference,latency%20while%20preserving%20output%20quality.](https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/#:~:text=Speculative%20decoding%20is%20an%20inference,latency%20while%20preserving%20output%20quality.)  
76. Speculative decoding | LLM Inference Handbook \- BentoML, 访问时间为 十一月 5, 2025， [https://bentoml.com/llm/inference-optimization/speculative-decoding](https://bentoml.com/llm/inference-optimization/speculative-decoding)  
77. Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2401.07851v3](https://arxiv.org/html/2401.07851v3)  
78. hemingkx/SpeculativeDecodingPapers: Must-read papers and blogs on Speculative Decoding ⚡️ \- GitHub, 访问时间为 十一月 5, 2025， [https://github.com/hemingkx/SpeculativeDecodingPapers](https://github.com/hemingkx/SpeculativeDecodingPapers)  
79. Fast Inference from Transformers via Speculative Decoding \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/pdf?id=C9NEblP8vS](https://openreview.net/pdf?id=C9NEblP8vS)  
80. \[2307.03172\] Lost in the Middle: How Language Models Use Long Contexts \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)  
81. Practical AI/ML Paper reading： “Lost in the Middle”: How Language Models Use Long Contexts | by Christmas Carol | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@carolzhu/lost-in-the-middle-how-language-models-use-long-contexts-2891830f8000](https://medium.com/@carolzhu/lost-in-the-middle-how-language-models-use-long-contexts-2891830f8000)  
82. Lost in the Middle: How Language Models Use Long Contexts \- ACL Anthology, 访问时间为 十一月 5, 2025， [https://aclanthology.org/2024.tacl-1.9/](https://aclanthology.org/2024.tacl-1.9/)  
83. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)  
84. 访问时间为 十一月 5, 2025， [https://www.k2view.com/blog/chain-of-thought-reasoning/\#:\~:text=Chain%2Dof%2Dthought%20reasoning%20is,results%20in%20more%20accurate%20responses.](https://www.k2view.com/blog/chain-of-thought-reasoning/#:~:text=Chain%2Dof%2Dthought%20reasoning%20is,results%20in%20more%20accurate%20responses.)  
85. Chain-of-Thought Prompting | Prompt Engineering Guide, 访问时间为 十一月 5, 2025， [https://www.promptingguide.ai/techniques/cot](https://www.promptingguide.ai/techniques/cot)  
86. Chain-of-Thought Prompting: A Comprehensive Analysis of Reasoning Techniques in Large Language Models | by Pier-Jean Malandrino | Scub-Lab, 访问时间为 十一月 5, 2025， [https://lab.scub.net/chain-of-thought-prompting-a-comprehensive-analysis-of-reasoning-techniques-in-large-language-b67fdd2eb72a](https://lab.scub.net/chain-of-thought-prompting-a-comprehensive-analysis-of-reasoning-techniques-in-large-language-b67fdd2eb72a)  
87. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2201.11903](https://arxiv.org/pdf/2201.11903)  
88. What is chain of thought (CoT) prompting? \- IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/chain-of-thoughts](https://www.ibm.com/think/topics/chain-of-thoughts)  
89. What is Tree Of Thoughts Prompting? \- IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/tree-of-thoughts](https://www.ibm.com/think/topics/tree-of-thoughts)  
90. Tree of Thoughts: Deliberate Problem Solving with Large Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2305.10601](https://arxiv.org/pdf/2305.10601)  
91. Paper Explained: Tree of Thoughts — Deliberate Problem Solving with Large Language Models | by Isaac Kargar, 访问时间为 十一月 5, 2025， [https://kargarisaac.medium.com/paper-explained-tree-of-thoughts-deliberate-problem-solving-with-large-language-models-46958bef0319](https://kargarisaac.medium.com/paper-explained-tree-of-thoughts-deliberate-problem-solving-with-large-language-models-46958bef0319)  
92. 访问时间为 十一月 5, 2025， [https://docs.opengauss.org/zh/docs/7.0.0-RC2/docs/AIFeatureGuide/%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E6%8A%80%E6%9C%AF.html\#:\~:text=LLM%20Agent%E6%98%AF%E4%B8%80%E7%A7%8D,%E6%B5%81%EF%BC%8C%E4%B8%AD%E9%97%B4%E6%B2%A1%E6%9C%89%E4%BA%BA%E5%B7%A5%E4%BB%8B%E5%85%A5%E3%80%82](https://docs.opengauss.org/zh/docs/7.0.0-RC2/docs/AIFeatureGuide/%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E6%8A%80%E6%9C%AF.html#:~:text=LLM%20Agent%E6%98%AF%E4%B8%80%E7%A7%8D,%E6%B5%81%EF%BC%8C%E4%B8%AD%E9%97%B4%E6%B2%A1%E6%9C%89%E4%BA%BA%E5%B7%A5%E4%BB%8B%E5%85%A5%E3%80%82)  
93. 大语言模型驱动的智能体研究进展与应用前景（2024） \- 皮书数据库, 访问时间为 十一月 5, 2025， [https://www.pishu.com.cn/skwx\_ps/initDatabaseDetail?contentId=15675097\&siteId=14\&contentType=literature](https://www.pishu.com.cn/skwx_ps/initDatabaseDetail?contentId=15675097&siteId=14&contentType=literature)  
94. LLM 智能体编排：分步指南| IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/cn-zh/think/tutorials/llm-agent-orchestration-with-langchain-and-granite](https://www.ibm.com/cn-zh/think/tutorials/llm-agent-orchestration-with-langchain-and-granite)  
95. \[2210.03629\] ReAct: Synergizing Reasoning and Acting in Language Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)  
96. ReAct: Synergizing Reasoning and Acting in Language Models \- Google Research, 访问时间为 十一月 5, 2025， [https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/)  
97. What is a ReAct Agent? | IBM, 访问时间为 十一月 5, 2025， [https://www.ibm.com/think/topics/react-agent](https://www.ibm.com/think/topics/react-agent)  
98. ReAct Paper Explained Simply: How Language Models Can Think and Act \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@deejairesearcher/react-paper-explained-simply-how-language-models-can-think-and-act-f500395f88db](https://medium.com/@deejairesearcher/react-paper-explained-simply-how-language-models-can-think-and-act-f500395f88db)  
99. Toolformer: Language Models Can Teach Themselves to Use Tools \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/pdf?id=Yacmpz84TH](https://openreview.net/pdf?id=Yacmpz84TH)  
100. arXiv:2302.04761v1 \[cs.CL\] 9 Feb 2023, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2302.04761](https://arxiv.org/pdf/2302.04761)  
101. Toolformer: Language Models Can Teach Themselves to Use Tools | Research \- AI at Meta, 访问时间为 十一月 5, 2025， [https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/](https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/)  
102. Toolformer: Language Models Can Teach Themselves to Use Tools \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=Yacmpz84TH](https://openreview.net/forum?id=Yacmpz84TH)  
103. joonspk-research/generative\_agents: Generative Agents: Interactive Simulacra of Human Behavior \- GitHub, 访问时间为 十一月 5, 2025， [https://github.com/joonspk-research/generative\_agents](https://github.com/joonspk-research/generative_agents)  
104. \[2304.03442\] Generative Agents: Interactive Simulacra of Human Behavior \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442)  
105. Generative Agents: Interactive Simulacra of Human Behavior Paper \- AI Agents forming Relationships and Planning activites : r/singularity \- Reddit, 访问时间为 十一月 5, 2025， [https://www.reddit.com/r/singularity/comments/12i0qyb/generative\_agents\_interactive\_simulacra\_of\_human/](https://www.reddit.com/r/singularity/comments/12i0qyb/generative_agents_interactive_simulacra_of_human/)  
106. Generative Agents: Interactive Simulacra of Human Behavior \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2304.03442](https://arxiv.org/pdf/2304.03442)  
107. Paper Review: Generative Agents: Interactive Simulacra of Human Behavior, 访问时间为 十一月 5, 2025， [https://artgor.medium.com/paper-review-generative-agents-interactive-simulacra-of-human-behavior-cc5f8294b4ac](https://artgor.medium.com/paper-review-generative-agents-interactive-simulacra-of-human-behavior-cc5f8294b4ac)  
108. 访问时间为 十一月 5, 2025， [https://edu.aliyun.com/trainingcamp/358964/\#:\~:text=AIGC%20%E2%80%9CAIGC%E2%80%9D%E6%98%AF%E2%80%9C%E4%BA%BA%E5%B7%A5,%E6%95%88%E6%9E%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%92%8C%E9%9F%B3%E9%A2%91%E3%80%82](https://edu.aliyun.com/trainingcamp/358964/#:~:text=AIGC%20%E2%80%9CAIGC%E2%80%9D%E6%98%AF%E2%80%9C%E4%BA%BA%E5%B7%A5,%E6%95%88%E6%9E%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%92%8C%E9%9F%B3%E9%A2%91%E3%80%82)  
109. 探析AIGC 的技术发展和应用, 访问时间为 十一月 5, 2025， [https://pdf.dfcfw.com/pdf/H3\_AP202302131583073056\_1.pdf](https://pdf.dfcfw.com/pdf/H3_AP202302131583073056_1.pdf)  
110. Denoising Diffusion Probabilistic Models | by Eleventh Hour Enthusiast | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@EleventhHourEnthusiast/denoising-diffusion-probabilistic-models-63b4fd3a3b67](https://medium.com/@EleventhHourEnthusiast/denoising-diffusion-probabilistic-models-63b4fd3a3b67)  
111. Biomedical Image Segmentation Using Denoising Diffusion Probabilistic Models: A Comprehensive Review and Analysis \- MDPI, 访问时间为 十一月 5, 2025， [https://www.mdpi.com/2076-3417/14/2/632](https://www.mdpi.com/2076-3417/14/2/632)  
112. InDepth Guide to Denoising Diffusion Probabilistic Models DDPM \- Learn OpenCV, 访问时间为 十一月 5, 2025， [https://learnopencv.com/denoising-diffusion-probabilistic-models/](https://learnopencv.com/denoising-diffusion-probabilistic-models/)  
113. \[2112.10752\] High-Resolution Image Synthesis with Latent Diffusion Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)  
114. Latent Diffusion Models: A Review — Part I | by Sertis \- Medium, 访问时间为 十一月 5, 2025， [https://sertiscorp.medium.com/latent-diffusion-models-a-review-part-i-d0feacc4906](https://sertiscorp.medium.com/latent-diffusion-models-a-review-part-i-d0feacc4906)  
115. High-Resolution Image Synthesis with Latent Diffusion Models \- Computer Vision & Learning Group \- Ommer-Lab, 访问时间为 十一月 5, 2025， [https://ommer-lab.com/research/latent-diffusion-models/](https://ommer-lab.com/research/latent-diffusion-models/)  
116. Adding Conditional Control to Text-to-Image Diffusion Models \- CVF Open Access, 访问时间为 十一月 5, 2025， [https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang\_Adding\_Conditional\_Control\_to\_Text-to-Image\_Diffusion\_Models\_ICCV\_2023\_paper.pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)  
117. Paper page \- Adding Conditional Control to Text-to-Image Diffusion Models \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/papers/2302.05543](https://huggingface.co/papers/2302.05543)  
118. \[2302.05543\] Adding Conditional Control to Text-to-Image Diffusion Models \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)  
119. lllyasviel/ControlNet: Let us control diffusion models\! \- GitHub, 访问时间为 十一月 5, 2025， [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)  
120. LLaVA-MORE : A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2503.15621v2](https://arxiv.org/html/2503.15621v2)  
121. Multimodal Large Language Models (MLLMs) transforming Computer Vision \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@tenyks\_blogger/multimodal-large-language-models-mllms-transforming-computer-vision-76d3c5dd267f](https://medium.com/@tenyks_blogger/multimodal-large-language-models-mllms-transforming-computer-vision-76d3c5dd267f)  
122. \[PDF\] Learning Transferable Visual Models From Natural Language Supervision, 访问时间为 十一月 5, 2025， [https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  
123. Learning Transferable Visual Models From Natural Language Supervision | by Eleventh Hour Enthusiast | Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@EleventhHourEnthusiast/learning-transferable-visual-models-from-natural-language-supervision-9d4cc9bc9af2](https://medium.com/@EleventhHourEnthusiast/learning-transferable-visual-models-from-natural-language-supervision-9d4cc9bc9af2)  
124. Learning Transferable Visual Models From Natural Language Supervision, 访问时间为 十一月 5, 2025， [https://proceedings.mlr.press/v139/radford21a/radford21a.pdf](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)  
125. LLaVa and Visual Instruction Tuning Explained \- Zilliz blog, 访问时间为 十一月 5, 2025， [https://zilliz.com/blog/llava-visual-instruction-training](https://zilliz.com/blog/llava-visual-instruction-training)  
126. Learning Transferable Visual Models From Natural Language Supervision \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2103.00020](https://arxiv.org/pdf/2103.00020)  
127. \[2304.08485\] Visual Instruction Tuning \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2304.08485](https://arxiv.org/abs/2304.08485)  
128. Visual Instruction Tuning, Large Language and Vision Assistant (LLaVA) \- Medium, 访问时间为 十一月 5, 2025， [https://medium.com/@aadicodes/visual-instruction-tuning-large-language-and-vision-assistant-llava-4cfbe3f02efb](https://medium.com/@aadicodes/visual-instruction-tuning-large-language-and-vision-assistant-llava-4cfbe3f02efb)  
129. How Multimodal LLMs Work \- The Vision Story \- Analytics Vidhya, 访问时间为 十一月 5, 2025， [https://www.analyticsvidhya.com/blog/2025/06/multimodal-llm/](https://www.analyticsvidhya.com/blog/2025/06/multimodal-llm/)  
130. 访问时间为 十一月 5, 2025， [https://www.researchgate.net/publication/354058115\_Applied\_AI\_matters\_AI4Code\_applying\_artificial\_intelligence\_to\_source\_code\#:\~:text=Large%20language%20models%20(LLMs)%20have,as%20it%20is%20being%20written.](https://www.researchgate.net/publication/354058115_Applied_AI_matters_AI4Code_applying_artificial_intelligence_to_source_code#:~:text=Large%20language%20models%20\(LLMs\)%20have,as%20it%20is%20being%20written.)  
131. Applied AI matters: AI4Code: applying artificial intelligence to source code \- ResearchGate, 访问时间为 十一月 5, 2025， [https://www.researchgate.net/publication/354058115\_Applied\_AI\_matters\_AI4Code\_applying\_artificial\_intelligence\_to\_source\_code](https://www.researchgate.net/publication/354058115_Applied_AI_matters_AI4Code_applying_artificial_intelligence_to_source_code)  
132. Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review \- MDPI, 访问时间为 十一月 5, 2025， [https://www.mdpi.com/1099-4300/25/6/888](https://www.mdpi.com/1099-4300/25/6/888)  
133. A Systematic Survey on Large Language Models for Static Code Analysis \- ARO-THE SCIENTIFIC JOURNAL OF KOYA UNIVERSITY, 访问时间为 十一月 5, 2025， [https://bp.koyauniversity.org/index.php/aro/article/download/2082/468](https://bp.koyauniversity.org/index.php/aro/article/download/2082/468)  
134. A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2311.10372](https://arxiv.org/abs/2311.10372)  
135. A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2311.10372v2](https://arxiv.org/html/2311.10372v2)  
136. A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends, 访问时间为 十一月 5, 2025， [https://www.semanticscholar.org/paper/A-Survey-of-Large-Language-Models-for-Code%3A-and-Zheng-Ning/3793e9bb830cb6961668e4c2fd33cd8d524c6308](https://www.semanticscholar.org/paper/A-Survey-of-Large-Language-Models-for-Code%3A-and-Zheng-Ning/3793e9bb830cb6961668e4c2fd33cd8d524c6308)  
137. WizardCoder: Empowering Code Large Language Models with Evol-Instruct \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/forum?id=UnUwSIgK5W](https://openreview.net/forum?id=UnUwSIgK5W)  
138. WizardCoder: Empowering Code Large Language Models with Evol-Instruct \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/html/2306.08568v1](https://arxiv.org/html/2306.08568v1)  
139. WizardCoder: EMPOWERING CODE LARGE LAN- \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/pdf?id=UnUwSIgK5W](https://openreview.net/pdf?id=UnUwSIgK5W)  
140. WizardCoder: Why It's the Best Coding Model Out There | Towards AI, 访问时间为 十一月 5, 2025， [https://towardsai.net/p/artificial-intelligence/wizardcoder-why-its-the-best-coding-model-out-there](https://towardsai.net/p/artificial-intelligence/wizardcoder-why-its-the-best-coding-model-out-there)  
141. WizardCoder: Empowering Code Large Language Models with Evol-Instruct \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/pdf/2306.08568](https://arxiv.org/pdf/2306.08568)  
142. Daily Papers \- Hugging Face, 访问时间为 十一月 5, 2025， [https://huggingface.co/papers?q=low-level%20module](https://huggingface.co/papers?q=low-level+module)  
143. AI Distilled | 0 articles | Packt Learning Hub, 访问时间为 十一月 5, 2025， [https://www.packtpub.com/fr-nl/learning/aidistilled?orderBy=most-viewed\&page=2](https://www.packtpub.com/fr-nl/learning/aidistilled?orderBy=most-viewed&page=2)  
144. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \- arXiv, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)  
145. ELI5: FlashAttention. Step by step explanation of how one of… \- Aleksa Gordić, 访问时间为 十一月 5, 2025， [https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)  
146. FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness \- OpenReview, 访问时间为 十一月 5, 2025， [https://openreview.net/pdf?id=H4DqfPSibmx](https://openreview.net/pdf?id=H4DqfPSibmx)  
147. Paper Summary \#8 \- FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, 访问时间为 十一月 5, 2025， [https://shreyansh26.github.io/post/2023-03-26\_flash-attention/](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)  
148. \[1910.02054\] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, 访问时间为 十一月 5, 2025， [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054)  
149. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models \- aiichironakano, 访问时间为 十一月 5, 2025， [https://aiichironakano.github.io/cs596/Rajbhandari-ZeRO-SC20.pdf](https://aiichironakano.github.io/cs596/Rajbhandari-ZeRO-SC20.pdf)  
150. \[2019 SC\] ZeRO: memory optimizations toward training trillion parameter models | Rui's Blog, 访问时间为 十一月 5, 2025， [https://blog.ruipan.xyz/machine-learning-systems/machine-learning-systems-index/2019-sc-zero-memory-optimizations-toward-training-trillion-parameter-models](https://blog.ruipan.xyz/machine-learning-systems/machine-learning-systems-index/2019-sc-zero-memory-optimizations-toward-training-trillion-parameter-models)  
151. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models \- Microsoft, 访问时间为 十一月 5, 2025， [https://www.microsoft.com/en-us/research/publication/zero-memory-optimizations-toward-training-trillion-parameter-models/](https://www.microsoft.com/en-us/research/publication/zero-memory-optimizations-toward-training-trillion-parameter-models/)